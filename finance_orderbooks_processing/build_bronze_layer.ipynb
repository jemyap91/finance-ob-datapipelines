{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ceea2a0-c7fe-45d5-88d5-30ce218ac11c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6683a705-9963-476d-b7cc-2bc94b96ec8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c97c9f0-7c77-40ee-b557-50e14740f339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, re, uuid, traceback, pandas as pd\n",
    "import hashlib\n",
    "from typing import List, Optional, Tuple\n",
    "from datetime import datetime, date\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col as F_col, count, when, upper, trim\n",
    "import yaml\n",
    "from data_extraction import read_file\n",
    "from data_utils import list_volume_files\n",
    "from data_processing import process_excel_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6691c924-38ef-48ef-80ff-c79b667e827d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG orderbooks_main;\n",
    "CREATE SCHEMA IF NOT EXISTS bronze;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS bronze.orderbook_data (\n",
    "  JobNumber                       STRING,\n",
    "  Office                          STRING,\n",
    "  office_div                      STRING,\n",
    "  ProjectTitle                    STRING,\n",
    "  Client                          STRING,\n",
    "  location_country                STRING,\n",
    "  gross_fee_usd                   DECIMAL(18,2),\n",
    "  fee_earned_usd                  DECIMAL(18,2),\n",
    "  gross_fee_yet_to_be_earned_usd  DECIMAL(18,2),\n",
    "  Currency                        STRING,\n",
    "  GrossFee                        DECIMAL(18,2),\n",
    "  GrossFeeEarned                  DECIMAL(18,2),\n",
    "  GrossFeeYetToBeEarned           DECIMAL(18,2),\n",
    "  Status                          STRING,\n",
    "  NewProject                      INT,\n",
    "  StartDate                       DATE,\n",
    "  anticipated_end_date            DATE,\n",
    "  ProjectType                     STRING,\n",
    "\n",
    "  -- metadata\n",
    "  source_file                     STRING,\n",
    "  source_mtime                    TIMESTAMP,\n",
    "  data_year                       INT,\n",
    "  data_month                      STRING,\n",
    "  data_month_num                  INT,\n",
    "  data_collection_date            DATE,\n",
    "  row_hash                        STRING\n",
    ")\n",
    "USING DELTA\n",
    "PARTITIONED BY (data_year, data_month_num)\n",
    "TBLPROPERTIES (\n",
    "  delta.autoOptimize.optimizeWrite = true,\n",
    "  delta.autoOptimize.autoCompact   = true\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04cdd633-4045-4297-89bd-d95cab9d66df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Access configuration\n",
    "CATALOG_NAME = config['catalog']['name']\n",
    "SCHEMA_NAME = config['catalog']['schema']\n",
    "TABLE_NAME = config['catalog']['table']\n",
    "PROCESSLOG_TABLE = config['catalog']['process_log_table']\n",
    "\n",
    "VOLUME_PATH = config['volume']['path']\n",
    "SOURCE_FOLDER = config['volume']['source_folder']\n",
    "\n",
    "VALID_EXTENSIONS = set(config['processing']['valid_extensions'])\n",
    "SCAN_ROWS = config['processing']['scan_rows']\n",
    "FILE_PREFIX_PATTERN = config['processing']['file_prefix_pattern']\n",
    "\n",
    "TARGET_COLUMNS = config['target_columns']\n",
    "COLUMN_ALIASES = config.get('column_aliases', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c22c8a7-5bd6-4d20-b93b-af3533f5b035",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "list_volume_files(dbutils, VOLUME_PATH, SOURCE_FOLDER, FILE_PREFIX_PATTERN, VALID_EXTENSIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c53abe3-210a-4565-aded-383b00fe95be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def normalize_column_names(df):\n",
    "    \"\"\"\n",
    "    Normalize pandas DataFrame column names to match the Delta table schema.\n",
    "    \"\"\"\n",
    "    column_mapping = {\n",
    "        'Office (Div)': 'office_div',\n",
    "        'Location (Country)': 'location_country',\n",
    "        'Gross Fee (USD)': 'gross_fee_usd',\n",
    "        'Fee Earned (USD)': 'fee_earned_usd',\n",
    "        'Gross Fee Yet To Be Earned (USD)': 'gross_fee_yet_to_be_earned_usd',\n",
    "        'Anticipated EndDate': 'anticipated_end_date',\n",
    "        'StartDate': 'StartDate',\n",
    "        'ProjectType': 'ProjectType'\n",
    "    }\n",
    "    \n",
    "    # Rename columns\n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Convert column names to lowercase where needed\n",
    "    final_columns = {}\n",
    "    for col in df.columns:\n",
    "        if col in ['JobNumber', 'Office', 'ProjectTitle', 'Client', 'Currency', \n",
    "                   'GrossFee', 'GrossFeeEarned', 'GrossFeeYetToBeEarned', \n",
    "                   'Status', 'NewProject', 'StartDate', 'ProjectType']:\n",
    "            final_columns[col] = col\n",
    "        else:\n",
    "            final_columns[col] = col.lower()\n",
    "    \n",
    "    df = df.rename(columns=final_columns)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98f0d291-75ef-4dc2-a107-e83b356a887d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(f\"ORDER BOOK DATA EXTRACTION PIPELINE\")\n",
    "print(f\"Volume: {VOLUME_PATH}/{SOURCE_FOLDER}\")\n",
    "print(f\"Target: {CATALOG_NAME}.{SCHEMA_NAME}.{TABLE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7a1d51f-f3ef-4621-8d1b-98bacb15b05c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "matching_files = list_volume_files(dbutils, VOLUME_PATH, SOURCE_FOLDER, FILE_PREFIX_PATTERN, VALID_EXTENSIONS)\n",
    "\n",
    "print(f\"\\n✓ Found {len(matching_files)} matching files:\")\n",
    "for f in matching_files:\n",
    "    print(f\"  - {f['name']} ({f['size']:,} bytes, modified: {f['mtime']})\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4ba68bb-086e-4e3b-817e-065ff33b0cb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Process files, call process_excel_file\n",
    "\n",
    "all_dataframes = []\n",
    "successful_files = []\n",
    "failed_files = []\n",
    "\n",
    "for file_info in matching_files:\n",
    "    try:\n",
    "        df = process_excel_file(file_info)\n",
    "        \n",
    "        if not df.empty:\n",
    "            all_dataframes.append(df)\n",
    "            successful_files.append(file_info['name'])\n",
    "        else:\n",
    "            failed_files.append((file_info['name'], \"No data extracted\"))\n",
    "            \n",
    "    except Exception as e:\n",
    "        failed_files.append((file_info['name'], str(e)))\n",
    "        print(f\"✗ Failed to process {file_info['name']}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e00b9a49-1110-4c26-a147-8152b554b6ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "print(f\"✓ Total rows before normalization: {len(combined_df)}\")\n",
    "\n",
    "# Normalize column names to match Delta table schema\n",
    "combined_df = normalize_column_names(combined_df)\n",
    "\n",
    "# Convert date columns properly\n",
    "date_columns = ['StartDate', 'anticipated_end_date', 'data_collection_date']\n",
    "for col_name in date_columns:\n",
    "    if col_name in combined_df.columns:\n",
    "        # Try multiple date formats commonly found in Excel\n",
    "        # First try YYYY-MM-DD, then DD/MM/YYYY, then let pandas infer\n",
    "        def parse_date_flexible(date_val):\n",
    "            if pd.isna(date_val) or date_val is None or date_val == '':\n",
    "                return None\n",
    "            \n",
    "            # Try YYYY-MM-DD format first\n",
    "            try:\n",
    "                return pd.to_datetime(date_val, format='%Y-%m-%d', errors='raise')\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Try DD/MM/YYYY format\n",
    "            try:\n",
    "                return pd.to_datetime(date_val, format='%d/%m/%Y', errors='raise')\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Try MM/DD/YYYY format\n",
    "            try:\n",
    "                return pd.to_datetime(date_val, format='%m/%d/%Y', errors='raise')\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Let pandas infer as last resort\n",
    "            try:\n",
    "                return pd.to_datetime(date_val, errors='coerce')\n",
    "            except:\n",
    "                return None\n",
    "        \n",
    "        combined_df[col_name] = combined_df[col_name].apply(parse_date_flexible)\n",
    "        # Convert datetime to date objects\n",
    "        combined_df[col_name] = combined_df[col_name].dt.date\n",
    "        # Replace NaT with None\n",
    "        combined_df[col_name] = combined_df[col_name].where(pd.notna(combined_df[col_name]), None)\n",
    "\n",
    "integer_columns = ['data_year', 'data_month_num']\n",
    "for col_name in integer_columns:\n",
    "    if col_name in combined_df.columns:\n",
    "        # Convert to nullable Int64 type to handle NaN values\n",
    "        combined_df[col_name] = pd.to_numeric(combined_df[col_name], errors='coerce').astype('Int64')\n",
    "        # Replace NaN with None for Spark\n",
    "        combined_df[col_name] = combined_df[col_name].where(pd.notna(combined_df[col_name]), None)\n",
    "\n",
    "print(f\"✓ Columns after normalization: {list(combined_df.columns)}\")\n",
    "print(f\"✓ Total rows: {len(combined_df)}\")\n",
    "print(f\"✓ Date column types:\")\n",
    "for col_name in date_columns:\n",
    "    if col_name in combined_df.columns:\n",
    "        print(f\"  - {col_name}: {combined_df[col_name].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddaee23b-f5ab-4f77-bb85-f4c6d971fd6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Properly convert numeric columns to float before Spark conversion\n",
    "decimal_columns = [\n",
    "    'gross_fee_usd', 'fee_earned_usd', 'gross_fee_yet_to_be_earned_usd',\n",
    "    'GrossFee', 'GrossFeeEarned', 'GrossFeeYetToBeEarned'\n",
    "]\n",
    "\n",
    "for col_name in decimal_columns:\n",
    "    if col_name in combined_df.columns:\n",
    "        # Convert to numeric, coerce errors to NaN, then to nullable float\n",
    "        combined_df[col_name] = pd.to_numeric(combined_df[col_name], errors='coerce')\n",
    "        # Fill NaN with None for proper NULL handling in Spark\n",
    "        combined_df[col_name] = combined_df[col_name].where(pd.notna(combined_df[col_name]), None)\n",
    "\n",
    "# Ensure string columns are proper strings\n",
    "string_columns = [\n",
    "    'JobNumber', 'Office', 'office_div', 'ProjectTitle', 'Client', \n",
    "    'location_country', 'Currency', 'Status', 'NewProject', 'ProjectType',\n",
    "    'source_file', 'data_month', 'row_hash'\n",
    "]\n",
    "\n",
    "for col_name in string_columns:\n",
    "    if col_name in combined_df.columns:\n",
    "        combined_df[col_name] = combined_df[col_name].astype(str)\n",
    "        combined_df[col_name] = combined_df[col_name].replace('nan', None)\n",
    "        combined_df[col_name] = combined_df[col_name].replace('None', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a15956f9-28a0-4fff-bcb1-f7c180f2a941",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49df52e7-0425-40cd-84dd-51f8ab4ab287",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark DataFrame without strict schema first\n",
    "try:\n",
    "    spark_df = spark.createDataFrame(combined_df)\n",
    "    print(f\"✓ Spark DataFrame created with {spark_df.count()} rows\")\n",
    "    \n",
    "    # Cast decimal columns to proper decimal type in Spark\n",
    "    for col_name in decimal_columns:\n",
    "        if col_name in spark_df.columns:\n",
    "            spark_df = spark_df.withColumn(col_name, F_col(col_name).cast(DecimalType(18, 2)))\n",
    "            print(f\"✓ Decimal columns cast to DecimalType(18, 2)\")\n",
    "    \n",
    "    # Cast 1 if 'New Project', 0 otherwise\n",
    "    if 'NewProject' in spark_df.columns:\n",
    "        # Convert NewProject to binary: 1 if new project, 0 otherwise\n",
    "        spark_df = spark_df.withColumn(\n",
    "            'NewProject',\n",
    "            when(\n",
    "                (upper(trim(F_col('NewProject'))).isin(['New Project', 'NewProject'])),\n",
    "                1\n",
    "            ).otherwise(0)\n",
    "        )\n",
    "        print(f\"✓ 'NewProject' column converted to binary\")\n",
    "    # Force integer types right before write\n",
    "    spark_df = spark_df.withColumn('data_year', F_col('data_year').cast(IntegerType()))\n",
    "    spark_df = spark_df.withColumn('data_month_num', F_col('data_month_num').cast(IntegerType()))\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error creating Spark DataFrame: {e}\")\n",
    "    print(\"Attempting alternative conversion...\")\n",
    "    \n",
    "    # Alternative: Let Spark infer schema and cast later\n",
    "    spark_df = spark.createDataFrame(combined_df.astype(str))\n",
    "    \n",
    "    # Cast columns to proper types\n",
    "    for col_name in decimal_columns:\n",
    "        if col_name in spark_df.columns:\n",
    "            spark_df = spark_df.withColumn(col_name, F_col(col_name).cast(DecimalType(18, 2)))\n",
    "    \n",
    "    for col_name in string_columns:\n",
    "        if col_name in spark_df.columns:\n",
    "            spark_df = spark_df.withColumn(col_name, F_col(col_name).cast(StringType()))\n",
    "    \n",
    "    if 'data_year' in spark_df.columns:\n",
    "        spark_df = spark_df.withColumn('data_year', F_col('data_year').cast(IntegerType()))\n",
    "    if 'data_month_num' in spark_df.columns:\n",
    "        spark_df = spark_df.withColumn('data_month_num', F_col('data_month_num').cast(IntegerType()))\n",
    "    \n",
    "    print(f\"✓ Alternative conversion successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cbab61a-05cd-434c-aab7-f072f9baeb82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check spark_df schema\n",
    "spark_df.printSchema()\n",
    "\n",
    "# Check specific problematic columns\n",
    "for col_name in ['data_year', 'data_month_num', 'NewProject']:\n",
    "    col_type = str(spark_df.schema[col_name].dataType)\n",
    "    print(f\"{col_name}: {col_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0bf430f-c8b8-4e44-b5d2-1f6a8653f6d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Store parquet file\n",
    "parquet_path = \"/Volumes/orderbooks_main/raw_files/fy25/processed_orderbook.parquet\"\n",
    "spark_df.write.mode(\"overwrite\").parquet(parquet_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d49458dc-7d75-4820-8d80-d00ca575d950",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_full_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{TABLE_NAME}\"\n",
    "    \n",
    "try:\n",
    "    spark_df.write.mode(\"overwrite\").saveAsTable(table_full_name)\n",
    "    \n",
    "    final_count = spark.table(table_full_name).count()\n",
    "    \n",
    "    print(f\"\\n SUCCESS!\")\n",
    "    print(f\"✓ Data written to: {table_full_name}\")\n",
    "    print(f\"✓ Total records in table: {final_count:,}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error writing to Unity Catalog: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7054201650373055,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "build_bronze_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
