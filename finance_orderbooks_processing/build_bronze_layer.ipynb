{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6683a705-9963-476d-b7cc-2bc94b96ec8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c97c9f0-7c77-40ee-b557-50e14740f339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, re, uuid, traceback, pandas as pd\n",
    "import hashlib\n",
    "from typing import List, Optional, Tuple\n",
    "from datetime import datetime, date\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col as F_col, count, when, upper, trim\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba7d35a-e31e-4469-af93-2d436d0ffca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./data_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6691c924-38ef-48ef-80ff-c79b667e827d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG orderbooks_main;\n",
    "CREATE SCHEMA IF NOT EXISTS bronze;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS bronze.orderbook_data (\n",
    "  JobNumber                       STRING,\n",
    "  Office                          STRING,\n",
    "  office_div                      STRING,\n",
    "  ProjectTitle                    STRING,\n",
    "  Client                          STRING,\n",
    "  location_country                STRING,\n",
    "  gross_fee_usd                   DECIMAL(18,2),\n",
    "  fee_earned_usd                  DECIMAL(18,2),\n",
    "  gross_fee_yet_to_be_earned_usd  DECIMAL(18,2),\n",
    "  Currency                        STRING,\n",
    "  GrossFee                        DECIMAL(18,2),\n",
    "  GrossFeeEarned                  DECIMAL(18,2),\n",
    "  GrossFeeYetToBeEarned           DECIMAL(18,2),\n",
    "  Status                          STRING,\n",
    "  NewProject                      INT,\n",
    "  StartDate                       DATE,\n",
    "  anticipated_end_date            DATE,\n",
    "  ProjectType                     STRING,\n",
    "\n",
    "  -- metadata\n",
    "  source_file                     STRING,\n",
    "  source_mtime                    TIMESTAMP,\n",
    "  data_year                       INT,\n",
    "  data_month                      STRING,\n",
    "  data_month_num                  INT,\n",
    "  data_collection_date            DATE,\n",
    "  row_hash                        STRING\n",
    ")\n",
    "USING DELTA\n",
    "PARTITIONED BY (data_year, data_month_num)\n",
    "TBLPROPERTIES (\n",
    "  delta.autoOptimize.optimizeWrite = true,\n",
    "  delta.autoOptimize.autoCompact   = true\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04cdd633-4045-4297-89bd-d95cab9d66df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Access configuration\n",
    "CATALOG_NAME = config['catalog']['name']\n",
    "SCHEMA_NAME = config['catalog']['schema']\n",
    "TABLE_NAME = config['catalog']['table']\n",
    "PROCESSLOG_TABLE = config['catalog']['process_log_table']\n",
    "\n",
    "VOLUME_PATH = config['volume']['path']\n",
    "SOURCE_FOLDER = config['volume']['source_folder']\n",
    "\n",
    "VALID_EXTENSIONS = set(config['processing']['valid_extensions'])\n",
    "SCAN_ROWS = config['processing']['scan_rows']\n",
    "FILE_PREFIX_PATTERN = config['processing']['file_prefix_pattern']\n",
    "\n",
    "TARGET_COLUMNS = config['target_columns']\n",
    "COLUMN_ALIASES = config.get('column_aliases', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb8ec38-97a7-462a-b992-7a7af9c8e64c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  Storage Configuration\n",
    "# STORAGE_ACCOUNT = \"financeorderbook\"\n",
    "# CONTAINER = \"orderbooksall\" \n",
    "# BASE_PATH = f\"abfss://{CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
    "\n",
    "# # Unity Catalog Configuration\n",
    "# CATALOG_NAME = \"orderbooks_main\"\n",
    "# SCHEMA_NAME = \"bronze\"\n",
    "# TABLE_NAME = \"orderbook_data\"\n",
    "# PROCESSLOG_TABLE = \"file_processing_log\"\n",
    "\n",
    "# # Volume path\n",
    "# VOLUME_PATH = \"/Volumes/orderbooks_main/raw_files\"\n",
    "# FOLDER = \"fy25\"\n",
    "\n",
    "# # File Processing Configuration\n",
    "# SOURCE_FOLDER = \"FY25\"  # Folder in your container with Excel files\n",
    "# VALID_EXTENSIONS = {\".xlsx\", \".xlsm\", \".xls\"}\n",
    "# SCAN_ROWS = 20\n",
    "# FILE_PREFIX_PATTERN = r\"^1[._]\"\n",
    "\n",
    "# # Target Columns (matching your existing script)\n",
    "# TARGET_COLUMNS = [\n",
    "#     'JobNumber', 'Office', 'Office (Div)', 'ProjectTitle', 'Client', \n",
    "#     'Location (Country)', 'Gross Fee (USD)', 'Fee Earned (USD)', \n",
    "#     'Gross Fee Yet To Be Earned (USD)', 'Currency', 'GrossFee', \n",
    "#     'GrossFeeEarned', 'GrossFeeYetToBeEarned', 'Status', 'NewProject', \n",
    "#     'StartDate', 'Anticipated EndDate', 'ProjectType'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dacd28c-df7a-412f-a35a-c1c92ddbccde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def list_volume_files(volume_path, folder, pattern, extensions):\n",
    "    \"\"\"\n",
    "    List all Excel files in the volume folder that match the prefix pattern.\n",
    "    \"\"\"\n",
    "    folder_path = f\"{volume_path}/{folder}\" if folder else volume_path\n",
    "    \n",
    "    try:\n",
    "        files = dbutils.fs.ls(folder_path)\n",
    "        \n",
    "        matching_files = []\n",
    "        for file_info in files:\n",
    "            filename = file_info.name\n",
    "            file_ext = os.path.splitext(filename)[1].lower()\n",
    "            \n",
    "            # Check if file matches pattern and has valid extension\n",
    "            if re.match(pattern, filename) and file_ext in extensions:\n",
    "                matching_files.append({\n",
    "                    'path': file_info.path,\n",
    "                    'name': filename,\n",
    "                    'size': file_info.size,\n",
    "                    'mtime': datetime.fromtimestamp(file_info.modificationTime / 1000)\n",
    "                })\n",
    "        \n",
    "        return matching_files\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing files: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c22c8a7-5bd6-4d20-b93b-af3533f5b035",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "list_volume_files(VOLUME_PATH, SOURCE_FOLDER, FILE_PREFIX_PATTERN, VALID_EXTENSIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c53abe3-210a-4565-aded-383b00fe95be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract year, month name, and month number from filename.\n",
    "    Examples:\n",
    "      \"1_Order_Book_Mar_2025.xlsx\" -> (2025, \"Mar\", 3)\n",
    "      \"1. Order Book (Jun 2025).xlsm\" -> (2025, \"Jun\", 6)\n",
    "    \"\"\"\n",
    "    month_map = {\n",
    "        'jan': (1, 'Jan'), 'january': (1, 'Jan'),\n",
    "        'feb': (2, 'Feb'), 'february': (2, 'Feb'),\n",
    "        'mar': (3, 'Mar'), 'march': (3, 'Mar'),\n",
    "        'apr': (4, 'Apr'), 'april': (4, 'Apr'),\n",
    "        'may': (5, 'May'),\n",
    "        'jun': (6, 'Jun'), 'june': (6, 'Jun'),\n",
    "        'jul': (7, 'Jul'), 'july': (7, 'Jul'),\n",
    "        'aug': (8, 'Aug'), 'august': (8, 'Aug'),\n",
    "        'sep': (9, 'Sep'), 'sept': (9, 'Sep'), 'september': (9, 'Sep'),\n",
    "        'oct': (10, 'Oct'), 'october': (10, 'Oct'),\n",
    "        'nov': (11, 'Nov'), 'november': (11, 'Nov'),\n",
    "        'dec': (12, 'Dec'), 'december': (12, 'Dec')\n",
    "    }\n",
    "    \n",
    "    # Extract year (4 digits)\n",
    "    year_match = re.search(r'20\\d{2}', filename)\n",
    "    year = int(year_match.group()) if year_match else None\n",
    "    \n",
    "    # Extract month\n",
    "    month_num = None\n",
    "    month_name = None\n",
    "    \n",
    "    filename_lower = filename.lower()\n",
    "    for month_key, (num, name) in month_map.items():\n",
    "        if month_key in filename_lower:\n",
    "            month_num = num\n",
    "            month_name = name\n",
    "            break\n",
    "    \n",
    "    return year, month_name, month_num\n",
    "\n",
    "def calculate_row_hash(row_dict):\n",
    "    \"\"\"\n",
    "    Calculate a hash for deduplication based on key fields.\n",
    "    \"\"\"\n",
    "    key_fields = ['JobNumber', 'ProjectTitle', 'Client', 'Office']\n",
    "    hash_string = '|'.join([str(row_dict.get(f, '')) for f in key_fields])\n",
    "    return hashlib.md5(hash_string.encode()).hexdigest()\n",
    "\n",
    "\n",
    "def process_excel_file(file_info):\n",
    "    \"\"\"\n",
    "    Process a single Excel file using data_extraction.py module.\n",
    "    \"\"\"\n",
    "    file_path = file_info['path']\n",
    "    file_name = file_info['name']\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Processing: {file_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Convert volume path to local path for pandas\n",
    "        # Volume paths are already accessible as local paths in Databricks\n",
    "        local_path = file_path.replace(\"dbfs:\", \"\")\n",
    "        print(local_path)\n",
    "        # Use your existing read_file function\n",
    "        df = read_file(local_path)\n",
    "        \n",
    "        if df.empty:\n",
    "            print(f\"⚠ No data extracted from {file_name}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(f\"✓ Successfully extracted {len(df)} rows\")\n",
    "        \n",
    "        # Add metadata columns\n",
    "        df['source_file'] = file_name\n",
    "        df['source_mtime'] = file_info['mtime']\n",
    "        \n",
    "        # Extract date information from filename\n",
    "        year, month_name, month_num = extract_date_from_filename(file_name)\n",
    "        df['data_year'] = year\n",
    "        df['data_month'] = month_name\n",
    "        df['data_month_num'] = month_num\n",
    "        \n",
    "        # Set collection date (file modification time or current date)\n",
    "        df['data_collection_date'] = file_info['mtime'].date()\n",
    "        \n",
    "        # Calculate row hash for each row\n",
    "        df['row_hash'] = df.apply(lambda row: calculate_row_hash(row.to_dict()), axis=1)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error processing {file_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def normalize_column_names(df):\n",
    "    \"\"\"\n",
    "    Normalize pandas DataFrame column names to match the Delta table schema.\n",
    "    \"\"\"\n",
    "    column_mapping = {\n",
    "        'Office (Div)': 'office_div',\n",
    "        'Location (Country)': 'location_country',\n",
    "        'Gross Fee (USD)': 'gross_fee_usd',\n",
    "        'Fee Earned (USD)': 'fee_earned_usd',\n",
    "        'Gross Fee Yet To Be Earned (USD)': 'gross_fee_yet_to_be_earned_usd',\n",
    "        'Anticipated EndDate': 'anticipated_end_date',\n",
    "        'StartDate': 'StartDate',\n",
    "        'ProjectType': 'ProjectType'\n",
    "    }\n",
    "    \n",
    "    # Rename columns\n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Convert column names to lowercase where needed\n",
    "    final_columns = {}\n",
    "    for col in df.columns:\n",
    "        if col in ['JobNumber', 'Office', 'ProjectTitle', 'Client', 'Currency', \n",
    "                   'GrossFee', 'GrossFeeEarned', 'GrossFeeYetToBeEarned', \n",
    "                   'Status', 'NewProject', 'StartDate', 'ProjectType']:\n",
    "            final_columns[col] = col\n",
    "        else:\n",
    "            final_columns[col] = col.lower()\n",
    "    \n",
    "    df = df.rename(columns=final_columns)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98f0d291-75ef-4dc2-a107-e83b356a887d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(f\"ORDER BOOK DATA EXTRACTION PIPELINE\")\n",
    "print(f\"Volume: {VOLUME_PATH}/{SOURCE_FOLDER}\")\n",
    "print(f\"Target: {CATALOG_NAME}.{SCHEMA_NAME}.{TABLE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7a1d51f-f3ef-4621-8d1b-98bacb15b05c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "matching_files = list_volume_files(VOLUME_PATH, SOURCE_FOLDER, FILE_PREFIX_PATTERN, VALID_EXTENSIONS)\n",
    "\n",
    "print(f\"\\n✓ Found {len(matching_files)} matching files:\")\n",
    "for f in matching_files:\n",
    "    print(f\"  - {f['name']} ({f['size']:,} bytes, modified: {f['mtime']})\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4ba68bb-086e-4e3b-817e-065ff33b0cb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_dataframes = []\n",
    "successful_files = []\n",
    "failed_files = []\n",
    "\n",
    "for file_info in matching_files:\n",
    "    try:\n",
    "        df = process_excel_file(file_info)\n",
    "        \n",
    "        if not df.empty:\n",
    "            all_dataframes.append(df)\n",
    "            successful_files.append(file_info['name'])\n",
    "        else:\n",
    "            failed_files.append((file_info['name'], \"No data extracted\"))\n",
    "            \n",
    "    except Exception as e:\n",
    "        failed_files.append((file_info['name'], str(e)))\n",
    "        print(f\"✗ Failed to process {file_info['name']}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e00b9a49-1110-4c26-a147-8152b554b6ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "print(f\"✓ Total rows before normalization: {len(combined_df)}\")\n",
    "\n",
    "# Normalize column names to match Delta table schema\n",
    "combined_df = normalize_column_names(combined_df)\n",
    "\n",
    "# Convert date columns properly\n",
    "date_columns = ['StartDate', 'anticipated_end_date', 'data_collection_date']\n",
    "for col_name in date_columns:\n",
    "    if col_name in combined_df.columns:\n",
    "        # Try multiple date formats commonly found in Excel\n",
    "        # First try YYYY-MM-DD, then DD/MM/YYYY, then let pandas infer\n",
    "        def parse_date_flexible(date_val):\n",
    "            if pd.isna(date_val) or date_val is None or date_val == '':\n",
    "                return None\n",
    "            \n",
    "            # Try YYYY-MM-DD format first\n",
    "            try:\n",
    "                return pd.to_datetime(date_val, format='%Y-%m-%d', errors='raise')\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Try DD/MM/YYYY format\n",
    "            try:\n",
    "                return pd.to_datetime(date_val, format='%d/%m/%Y', errors='raise')\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Try MM/DD/YYYY format\n",
    "            try:\n",
    "                return pd.to_datetime(date_val, format='%m/%d/%Y', errors='raise')\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Let pandas infer as last resort\n",
    "            try:\n",
    "                return pd.to_datetime(date_val, errors='coerce')\n",
    "            except:\n",
    "                return None\n",
    "        \n",
    "        combined_df[col_name] = combined_df[col_name].apply(parse_date_flexible)\n",
    "        # Convert datetime to date objects\n",
    "        combined_df[col_name] = combined_df[col_name].dt.date\n",
    "        # Replace NaT with None\n",
    "        combined_df[col_name] = combined_df[col_name].where(pd.notna(combined_df[col_name]), None)\n",
    "\n",
    "integer_columns = ['data_year', 'data_month_num']\n",
    "for col_name in integer_columns:\n",
    "    if col_name in combined_df.columns:\n",
    "        # Convert to nullable Int64 type to handle NaN values\n",
    "        combined_df[col_name] = pd.to_numeric(combined_df[col_name], errors='coerce').astype('Int64')\n",
    "        # Replace NaN with None for Spark\n",
    "        combined_df[col_name] = combined_df[col_name].where(pd.notna(combined_df[col_name]), None)\n",
    "\n",
    "print(f\"✓ Columns after normalization: {list(combined_df.columns)}\")\n",
    "print(f\"✓ Total rows: {len(combined_df)}\")\n",
    "print(f\"✓ Date column types:\")\n",
    "for col_name in date_columns:\n",
    "    if col_name in combined_df.columns:\n",
    "        print(f\"  - {col_name}: {combined_df[col_name].dtype}\")\n",
    "    \n",
    "print(f\"✓ Columns after normalization: {list(combined_df.columns)}\")\n",
    "print(f\"✓ Total rows: {len(combined_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddaee23b-f5ab-4f77-bb85-f4c6d971fd6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Properly convert numeric columns to float before Spark conversion\n",
    "decimal_columns = [\n",
    "    'gross_fee_usd', 'fee_earned_usd', 'gross_fee_yet_to_be_earned_usd',\n",
    "    'GrossFee', 'GrossFeeEarned', 'GrossFeeYetToBeEarned'\n",
    "]\n",
    "\n",
    "for col_name in decimal_columns:\n",
    "    if col_name in combined_df.columns:\n",
    "        # Convert to numeric, coerce errors to NaN, then to nullable float\n",
    "        combined_df[col_name] = pd.to_numeric(combined_df[col_name], errors='coerce')\n",
    "        # Fill NaN with None for proper NULL handling in Spark\n",
    "        combined_df[col_name] = combined_df[col_name].where(pd.notna(combined_df[col_name]), None)\n",
    "\n",
    "# Ensure string columns are proper strings\n",
    "string_columns = [\n",
    "    'JobNumber', 'Office', 'office_div', 'ProjectTitle', 'Client', \n",
    "    'location_country', 'Currency', 'Status', 'NewProject', 'ProjectType',\n",
    "    'source_file', 'data_month', 'row_hash'\n",
    "]\n",
    "\n",
    "for col_name in string_columns:\n",
    "    if col_name in combined_df.columns:\n",
    "        combined_df[col_name] = combined_df[col_name].astype(str)\n",
    "        combined_df[col_name] = combined_df[col_name].replace('nan', None)\n",
    "        combined_df[col_name] = combined_df[col_name].replace('None', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a15956f9-28a0-4fff-bcb1-f7c180f2a941",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49df52e7-0425-40cd-84dd-51f8ab4ab287",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark DataFrame without strict schema first\n",
    "try:\n",
    "    spark_df = spark.createDataFrame(combined_df)\n",
    "    print(f\"✓ Spark DataFrame created with {spark_df.count()} rows\")\n",
    "    \n",
    "    # Cast decimal columns to proper decimal type in Spark\n",
    "    for col_name in decimal_columns:\n",
    "        if col_name in spark_df.columns:\n",
    "            spark_df = spark_df.withColumn(col_name, F_col(col_name).cast(DecimalType(18, 2)))\n",
    "            print(f\"✓ Decimal columns cast to DecimalType(18, 2)\")\n",
    "    \n",
    "    # Cast 1 if 'New Project', 0 otherwise\n",
    "    if 'NewProject' in spark_df.columns:\n",
    "        # Convert NewProject to binary: 1 if new project, 0 otherwise\n",
    "        spark_df = spark_df.withColumn(\n",
    "            'NewProject',\n",
    "            when(\n",
    "                (upper(trim(F_col('NewProject'))).isin(['New Project', 'NewProject'])),\n",
    "                1\n",
    "            ).otherwise(0)\n",
    "        )\n",
    "        print(f\"✓ 'NewProject' column converted to binary\")\n",
    "    # Force integer types right before write\n",
    "    spark_df = spark_df.withColumn('data_year', F_col('data_year').cast(IntegerType()))\n",
    "    spark_df = spark_df.withColumn('data_month_num', F_col('data_month_num').cast(IntegerType()))\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error creating Spark DataFrame: {e}\")\n",
    "    print(\"Attempting alternative conversion...\")\n",
    "    \n",
    "    # Alternative: Let Spark infer schema and cast later\n",
    "    spark_df = spark.createDataFrame(combined_df.astype(str))\n",
    "    \n",
    "    # Cast columns to proper types\n",
    "    for col_name in decimal_columns:\n",
    "        if col_name in spark_df.columns:\n",
    "            spark_df = spark_df.withColumn(col_name, F_col(col_name).cast(DecimalType(18, 2)))\n",
    "    \n",
    "    for col_name in string_columns:\n",
    "        if col_name in spark_df.columns:\n",
    "            spark_df = spark_df.withColumn(col_name, F_col(col_name).cast(StringType()))\n",
    "    \n",
    "    if 'data_year' in spark_df.columns:\n",
    "        spark_df = spark_df.withColumn('data_year', F_col('data_year').cast(IntegerType()))\n",
    "    if 'data_month_num' in spark_df.columns:\n",
    "        spark_df = spark_df.withColumn('data_month_num', F_col('data_month_num').cast(IntegerType()))\n",
    "    \n",
    "    print(f\"✓ Alternative conversion successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cbab61a-05cd-434c-aab7-f072f9baeb82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check spark_df schema\n",
    "spark_df.printSchema()\n",
    "\n",
    "# Check specific problematic columns\n",
    "for col_name in ['data_year', 'data_month_num', 'NewProject']:\n",
    "    col_type = str(spark_df.schema[col_name].dataType)\n",
    "    print(f\"{col_name}: {col_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0bf430f-c8b8-4e44-b5d2-1f6a8653f6d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Store parquet file\n",
    "parquet_path = \"/Volumes/orderbooks_main/raw_files/fy25/processed_orderbook.parquet\"\n",
    "spark_df.write.mode(\"overwrite\").parquet(parquet_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d49458dc-7d75-4820-8d80-d00ca575d950",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_full_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{TABLE_NAME}\"\n",
    "    \n",
    "try:\n",
    "    spark_df.write.mode(\"overwrite\").saveAsTable(table_full_name)\n",
    "    \n",
    "    final_count = spark.table(table_full_name).count()\n",
    "    \n",
    "    print(f\"\\n SUCCESS!\")\n",
    "    print(f\"✓ Data written to: {table_full_name}\")\n",
    "    print(f\"✓ Total records in table: {final_count:,}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error writing to Unity Catalog: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7054201650373055,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "build_bronze_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
